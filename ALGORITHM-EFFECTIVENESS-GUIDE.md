# Data-Driven Algorithm Selection Guide

## ðŸŽ¯ The Right Approach: Test, Measure, Decide

You're absolutely correct - we CANNOT arbitrarily reduce 27 algorithms to 12 without data. We must be **scientific and evidence-based**.

---

## ðŸ“Š 3-Step Process

### Step 1: Fetch Real NIFTY/Options Data

```bash
cd /root/ankr-labs-nx/packages/vyomo-anomaly-agent

# Fetch 1 year of real NSE data
npx ts-node src/backtest/fetch-nse-data.ts
```

**What it fetches:**
- NIFTY 50 historical data (Yahoo Finance)
- Options chain data (NSE API)
- India VIX data
- 1 year = ~250 trading days = ~97,500 minute-level data points

**Output:**
- `data/real-market-data.json` - Raw market data
- `data/labeled-anomalies.csv` - Template for ground truth (you fill this in)

---

### Step 2: Label Ground Truth (Optional but Recommended)

Edit `data/labeled-anomalies.csv` and add known anomalies:

```csv
# date,symbol,type,severity,reason
2024-02-01,NIFTY,PRICE_SPIKE,CRITICAL,Budget announcement
2024-03-15,BANKNIFTY,VOLUME_SURGE,WARNING,Expiry day surge
2024-06-04,NIFTY,PRICE_DROP,CRITICAL,Election results
2024-07-23,NIFTY,PRICE_SPIKE,WARNING,RBI policy announcement
2024-09-10,BANKNIFTY,IV_SPIKE,WARNING,Geopolitical tensions
```

**Why label?**
- Gives us **ground truth** to measure accuracy
- Without labels: We use heuristics (less reliable)
- With labels: We get **true precision/recall**

**Where to find anomalies:**
- News archives (Economic Times, Moneycontrol)
- Your own trading logs
- Historical alert logs
- Known market events (Budget, RBI meetings, elections)

---

### Step 3: Run Backtest with ALL 27 Algorithms

```bash
# Run the full effectiveness analysis
npx ts-node src/backtest/run-backtest-real.ts
```

**What it does:**
1. âœ… Loads real market data
2. âœ… Runs ALL 27 algorithms
3. âœ… Tracks each algorithm's performance:
   - True Positives (caught real anomalies)
   - False Positives (false alarms)
   - False Negatives (missed anomalies)
   - Precision, Recall, F1 Score
   - Cost/benefit analysis
4. âœ… Generates recommendation: KEEP / TUNE / REMOVE

**Output:**
```
reports/algorithm-effectiveness.json

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ALGORITHM EFFECTIVENESS REPORT                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ“Š SUMMARY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Algorithms:      27
Overall Accuracy:      87.45%
Total Net Value:       â‚¹2,45,000

Recommendations:
  âœ… KEEP:    15 algorithms  (56%)
  âš ï¸  TUNE:   8 algorithms   (30%)
  âŒ REMOVE:  4 algorithms   (14%)

ðŸ† TOP 5 PERFORMERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. PRICE_SPIKE           F1: 92.3%  Value: â‚¹85,000
2. VOLUME_SURGE          F1: 88.7%  Value: â‚¹62,000
3. REVENGE_TRADING       F1: 85.1%  Value: â‚¹48,000
4. IV_SPIKE              F1: 82.4%  Value: â‚¹35,000
5. OVERTRADING           F1: 79.8%  Value: â‚¹28,000

âš ï¸  BOTTOM 5 PERFORMERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. NEWS_SENTIMENT        F1: 45.2%  Value: -â‚¹15,000  âŒ REMOVE
2. FEAR_GREED            F1: 48.7%  Value: -â‚¹8,000   âŒ REMOVE
3. TIME_ANOMALY          F1: 52.3%  Value: â‚¹2,000    âš ï¸  TUNE
4. PUT_CALL_RATIO        F1: 56.8%  Value: â‚¹5,000    âš ï¸  TUNE
5. WIN_STREAK_ESCALATION F1: 58.1%  Value: â‚¹7,000    âš ï¸  TUNE

ðŸ’¡ RECOMMENDATIONS

âŒ REMOVE THESE 4 ALGORITHMS:
  â€¢ NEWS_SENTIMENT
    Reason: F1 Score 45.2% (target: >50%), negative net value
  â€¢ FEAR_GREED
    Reason: Too many false positives, negative ROI
  â€¢ THETA_DECAY
    Reason: Low recall (35%), missing most anomalies
  â€¢ STRADDLE
    Reason: F1 Score 48%, not effective enough

âš ï¸  TUNE THESE 8 ALGORITHMS:
  â€¢ TIME_ANOMALY
    - Precision too low (62%) - reduce false positives
    - Suggestion: Increase detection threshold
  â€¢ PUT_CALL_RATIO
    - Recall too low (58%) - catching fewer anomalies
    - Suggestion: Decrease threshold or widen window
  â€¢ IV_SKEW
    - F1: 65% - needs threshold adjustment
  [... more details ...]

âœ… KEEP THESE 15 ALGORITHMS:
  â€¢ PRICE_SPIKE (F1: 92.3%)
  â€¢ VOLUME_SURGE (F1: 88.7%)
  â€¢ REVENGE_TRADING (F1: 85.1%)
  â€¢ IV_SPIKE (F1: 82.4%)
  [... 11 more ...]

ðŸš€ NEXT STEPS
1. Remove 4 ineffective algorithms
2. Tune 8 algorithms for better performance
3. Collect more labeled data for validation
4. Re-run backtest after tuning
5. Monitor production performance for 1-2 weeks
6. Iterate based on real production results
```

---

## ðŸ“ˆ Expected Results

Based on typical scenarios, you'll likely find:

### Scenario A: Most Algorithms Work (Good)
```
âœ… KEEP:   18-20 algorithms (67-74%)
âš ï¸  TUNE:  5-7 algorithms   (19-26%)
âŒ REMOVE: 2-4 algorithms   (7-15%)

Action: Minor cleanup, mostly tuning
Result: 23-25 effective algorithms
```

### Scenario B: Mixed Results (Normal)
```
âœ… KEEP:   12-15 algorithms (44-56%)
âš ï¸  TUNE:  8-10 algorithms  (30-37%)
âŒ REMOVE: 4-7 algorithms   (15-26%)

Action: Remove worst, tune middle, keep best
Result: 18-20 effective algorithms after tuning
```

### Scenario C: Many Don't Work (Bad Data/Thresholds)
```
âœ… KEEP:   8-10 algorithms  (30-37%)
âš ï¸  TUNE:  10-12 algorithms (37-44%)
âŒ REMOVE: 7-9 algorithms   (26-33%)

Action: Major tuning needed OR wrong thresholds
Result: Need to investigate WHY so many fail
```

---

## ðŸŽ¯ Decision Matrix

| F1 Score | Precision | Recall | Net Value | Decision |
|----------|-----------|--------|-----------|----------|
| >75% | >70% | >70% | Positive | âœ… **KEEP** |
| 50-75% | <70% | Any | Positive | âš ï¸  **TUNE** (Fix precision) |
| 50-75% | Any | <70% | Positive | âš ï¸  **TUNE** (Fix recall) |
| <50% | Any | Any | Negative | âŒ **REMOVE** |
| <50% | Any | Any | Positive but <â‚¹10k | âŒ **REMOVE** (Not worth it) |

**Cost Assumptions:**
- True Positive (catch real anomaly): **+â‚¹10,000** saved
- False Positive (false alarm): **-â‚¹500** wasted time
- False Negative (miss anomaly): **-â‚¹50,000** loss

---

## ðŸ”§ How to Tune Algorithms

### Problem 1: Low Precision (Too Many False Positives)

**Symptoms:**
- Algorithm fires too often
- Most alerts are false alarms
- Precision <70%

**Solutions:**
```typescript
// Increase threshold
OLD: threshold: 3.0  // 3 sigma
NEW: threshold: 3.5  // 3.5 sigma (fewer triggers)

// Narrow window
OLD: windowSize: 100
NEW: windowSize: 50  // More sensitive to recent changes

// Add confirmation
if (anomalyScore > threshold && volumeAlsoAnomalous) {
  // Only trigger if multiple signals agree
}
```

### Problem 2: Low Recall (Missing Too Many Anomalies)

**Symptoms:**
- Algorithm doesn't catch real issues
- Recall <70%

**Solutions:**
```typescript
// Decrease threshold
OLD: threshold: 3.0
NEW: threshold: 2.5  // More sensitive

// Widen window
OLD: windowSize: 50
NEW: windowSize: 100  // Capture more history

// Multiple detection methods
if (zScore > 2.5 || percentile > 95 || rollingMean > threshold) {
  // Trigger on any method
}
```

### Problem 3: Conflicting Signals

**Symptoms:**
- Algorithm disagrees with others
- Category alignment low

**Solutions:**
```typescript
// Add ensemble voting
const signals = [algo1(), algo2(), algo3()];
const majority = signals.filter(s => s === 'ANOMALY').length > 2;

if (majority) {
  // Only trigger if majority agrees
}
```

---

## ðŸ“ File Structure

```
vyomo-anomaly-agent/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ real-market-data.json          # Real NSE data (auto-generated)
â”‚   â””â”€â”€ labeled-anomalies.csv          # Ground truth (you fill this)
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ algorithm-effectiveness.json   # Full analysis report
â””â”€â”€ src/backtest/
    â”œâ”€â”€ fetch-nse-data.ts              # Step 1: Fetch real data
    â””â”€â”€ run-backtest-real.ts           # Step 2: Analyze effectiveness
```

---

## ðŸš€ Action Plan

### Today:
1. âœ… Fetch real NIFTY data: `npx ts-node src/backtest/fetch-nse-data.ts`
2. âš ï¸  Label 20-30 known anomalies in `data/labeled-anomalies.csv` (optional)
3. âœ… Run effectiveness analysis: `npx ts-node src/backtest/run-backtest-real.ts`

### This Week:
4. Review report: `reports/algorithm-effectiveness.json`
5. Remove algorithms marked as REMOVE
6. Tune algorithms marked as TUNE
7. Re-run backtest to validate improvements

### Next 2 Weeks:
8. Deploy to staging with tuned algorithms
9. Collect production data
10. Add more labeled anomalies
11. Re-run analysis monthly
12. Iterate based on real results

---

## ðŸ’¡ Key Insights

### Why This Approach Works:
- âœ… **Data-driven** - not guesswork
- âœ… **Measurable** - clear metrics
- âœ… **Iterative** - improves over time
- âœ… **Cost-aware** - considers ROI

### Why Random Cutting Doesn't Work:
- âŒ Might remove best algorithms
- âŒ Might keep worst algorithms
- âŒ No way to validate decisions
- âŒ No learning or improvement

### The Scientific Method:
1. **Hypothesis**: All 27 algorithms might not be necessary
2. **Test**: Run all 27 on real data
3. **Measure**: Calculate effectiveness metrics
4. **Analyze**: Identify which work, which don't
5. **Decide**: Keep/tune/remove based on data
6. **Iterate**: Re-test after changes

---

## ðŸŽ¯ Success Criteria

### Good Results:
- 18-23 algorithms remain (67-85% of original)
- Overall F1 score >80%
- Net value >â‚¹1,00,000 per year
- <10 false positives per day

### Warning Signs:
- <12 algorithms effective (<44%)
- Overall F1 score <70%
- Net value negative
- >50 false positives per day
- â†’ **Re-check thresholds or data quality**

---

## ðŸ¤ Next Steps

**Run the analysis now:**
```bash
# Step 1: Fetch data (5 minutes)
npx ts-node src/backtest/fetch-nse-data.ts

# Step 2: Run analysis (10 minutes)
npx ts-node src/backtest/run-backtest-real.ts

# Step 3: Review results
cat reports/algorithm-effectiveness.json
```

**Then share results and we'll decide together:**
- Which algorithms to keep
- Which to tune (and how)
- Which to remove
- Whether we need more data
- Whether thresholds need adjustment

---

## ðŸ“ž Support

If analysis shows:
- **>85% effective** â†’ You over-engineered slightly but it's working
- **50-85% effective** â†’ Normal, tune and iterate
- **<50% effective** â†’ Investigate data quality or threshold issues

**Let's be scientific about this!** ðŸ”¬

---

**Status:** Ready to run
**Next:** Execute Step 1 (fetch data) and share results
